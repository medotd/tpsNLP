{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading customer-support-on-twitter.zip to c:\\Users\\tech aarohi\\AppData\\Local\\Programs\\Microsoft VS Code\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/169M [00:00<?, ?B/s]\n",
      "  1%|          | 1.00M/169M [00:02<06:44, 435kB/s]\n",
      "  1%|          | 2.00M/169M [00:04<06:31, 446kB/s]\n",
      "  2%|▏         | 3.00M/169M [00:06<06:22, 454kB/s]\n",
      "  2%|▏         | 4.00M/169M [00:08<05:38, 510kB/s]\n",
      "  3%|▎         | 5.00M/169M [00:10<05:11, 550kB/s]\n",
      "  4%|▎         | 6.00M/169M [00:12<05:32, 513kB/s]\n",
      "  4%|▍         | 7.00M/169M [00:14<05:02, 560kB/s]\n",
      "  5%|▍         | 8.00M/169M [00:15<04:49, 581kB/s]\n",
      "  5%|▌         | 9.00M/169M [00:17<04:40, 597kB/s]\n",
      "  6%|▌         | 10.0M/169M [00:18<04:25, 625kB/s]\n",
      "  7%|▋         | 11.0M/169M [00:20<04:13, 652kB/s]\n",
      "  7%|▋         | 12.0M/169M [00:22<04:14, 645kB/s]\n",
      "  8%|▊         | 13.0M/169M [00:23<04:09, 655kB/s]\n",
      "  8%|▊         | 14.0M/169M [00:25<04:12, 642kB/s]\n",
      "  9%|▉         | 15.0M/169M [00:27<04:19, 622kB/s]\n",
      "  9%|▉         | 16.0M/169M [00:29<04:43, 565kB/s]\n",
      " 10%|█         | 17.0M/169M [00:31<04:40, 566kB/s]\n",
      " 11%|█         | 18.0M/169M [00:32<04:31, 581kB/s]\n",
      " 11%|█▏        | 19.0M/169M [00:34<04:14, 616kB/s]\n",
      " 12%|█▏        | 20.0M/169M [00:36<04:11, 619kB/s]\n",
      " 12%|█▏        | 21.0M/169M [00:37<04:08, 623kB/s]\n",
      " 13%|█▎        | 22.0M/169M [00:39<04:22, 586kB/s]\n",
      " 14%|█▎        | 23.0M/169M [00:41<04:35, 555kB/s]\n",
      " 14%|█▍        | 24.0M/169M [00:43<04:33, 554kB/s]\n",
      " 15%|█▍        | 25.0M/169M [00:45<04:24, 569kB/s]\n",
      " 15%|█▌        | 26.0M/169M [00:46<04:08, 602kB/s]\n",
      " 16%|█▌        | 27.0M/169M [00:48<03:58, 622kB/s]\n",
      " 17%|█▋        | 28.0M/169M [00:50<04:19, 568kB/s]\n",
      " 17%|█▋        | 29.0M/169M [00:52<04:00, 608kB/s]\n",
      " 18%|█▊        | 30.0M/169M [00:53<03:55, 618kB/s]\n",
      " 18%|█▊        | 31.0M/169M [00:55<03:43, 645kB/s]\n",
      " 19%|█▉        | 32.0M/169M [00:56<03:41, 647kB/s]\n",
      " 20%|█▉        | 33.0M/169M [00:58<03:33, 665kB/s]\n",
      " 20%|██        | 34.0M/169M [01:00<03:37, 648kB/s]\n",
      " 21%|██        | 35.0M/169M [01:01<03:36, 648kB/s]\n",
      " 21%|██▏       | 36.0M/169M [01:03<03:35, 644kB/s]\n",
      " 22%|██▏       | 37.0M/169M [01:04<03:30, 657kB/s]\n",
      " 23%|██▎       | 38.0M/169M [01:06<03:41, 617kB/s]\n",
      " 23%|██▎       | 39.0M/169M [01:08<03:55, 577kB/s]\n",
      " 24%|██▎       | 40.0M/169M [01:10<03:39, 613kB/s]\n",
      " 24%|██▍       | 41.0M/169M [01:12<03:41, 604kB/s]\n",
      " 25%|██▍       | 42.0M/169M [01:14<03:44, 590kB/s]\n",
      " 26%|██▌       | 43.0M/169M [01:16<03:51, 569kB/s]\n",
      " 26%|██▌       | 44.0M/169M [01:17<03:49, 570kB/s]\n",
      " 27%|██▋       | 45.0M/169M [01:19<03:42, 582kB/s]\n",
      " 27%|██▋       | 46.0M/169M [01:21<03:38, 587kB/s]\n",
      " 28%|██▊       | 47.0M/169M [01:22<03:30, 605kB/s]\n",
      " 28%|██▊       | 48.0M/169M [01:24<03:23, 621kB/s]\n",
      " 29%|██▉       | 49.0M/169M [01:26<03:20, 624kB/s]\n",
      " 30%|██▉       | 50.0M/169M [01:27<03:11, 648kB/s]\n",
      " 30%|███       | 51.0M/169M [01:29<03:08, 655kB/s]\n",
      " 31%|███       | 52.0M/169M [01:30<03:12, 636kB/s]\n",
      " 31%|███▏      | 53.0M/169M [01:32<03:17, 613kB/s]\n",
      " 32%|███▏      | 54.0M/169M [01:35<03:32, 565kB/s]\n",
      " 33%|███▎      | 55.0M/169M [01:36<03:26, 577kB/s]\n",
      " 33%|███▎      | 56.0M/169M [01:38<03:13, 610kB/s]\n",
      " 34%|███▍      | 57.0M/169M [01:39<03:09, 617kB/s]\n",
      " 34%|███▍      | 58.0M/169M [01:42<03:27, 560kB/s]\n",
      " 35%|███▍      | 59.0M/169M [01:43<03:18, 578kB/s]\n",
      " 36%|███▌      | 60.0M/169M [01:45<03:14, 584kB/s]\n",
      " 36%|███▌      | 61.0M/169M [01:47<03:17, 572kB/s]\n",
      " 37%|███▋      | 62.0M/169M [01:49<03:23, 550kB/s]\n",
      " 37%|███▋      | 63.0M/169M [01:51<03:07, 590kB/s]\n",
      " 38%|███▊      | 64.0M/169M [01:52<03:04, 593kB/s]\n",
      " 39%|███▊      | 65.0M/169M [01:54<02:58, 609kB/s]\n",
      " 39%|███▉      | 66.0M/169M [01:55<02:48, 638kB/s]\n",
      " 40%|███▉      | 67.0M/169M [01:57<02:42, 657kB/s]\n",
      " 40%|████      | 68.0M/169M [01:58<02:37, 670kB/s]\n",
      " 41%|████      | 69.0M/169M [02:00<02:39, 656kB/s]\n",
      " 42%|████▏     | 70.0M/169M [02:02<02:52, 601kB/s]\n",
      " 42%|████▏     | 71.0M/169M [02:04<02:41, 634kB/s]\n",
      " 43%|████▎     | 72.0M/169M [02:05<02:38, 640kB/s]\n",
      " 43%|████▎     | 73.0M/169M [02:07<02:54, 576kB/s]\n",
      " 44%|████▍     | 74.0M/169M [02:09<02:47, 593kB/s]\n",
      " 44%|████▍     | 75.0M/169M [02:11<02:37, 621kB/s]\n",
      " 45%|████▌     | 76.0M/169M [02:12<02:37, 616kB/s]\n",
      " 46%|████▌     | 77.0M/169M [02:14<02:37, 608kB/s]\n",
      " 46%|████▋     | 78.0M/169M [02:16<02:34, 615kB/s]\n",
      " 47%|████▋     | 79.0M/169M [02:18<02:59, 524kB/s]\n",
      " 47%|████▋     | 80.0M/169M [02:20<02:44, 565kB/s]\n",
      " 48%|████▊     | 81.0M/169M [02:22<02:46, 552kB/s]\n",
      " 49%|████▊     | 82.0M/169M [02:24<02:39, 570kB/s]\n",
      " 49%|████▉     | 83.0M/169M [02:26<02:43, 548kB/s]\n",
      " 50%|████▉     | 84.0M/169M [02:28<02:38, 559kB/s]\n",
      " 50%|█████     | 85.0M/169M [02:29<02:36, 561kB/s]\n",
      " 51%|█████     | 86.0M/169M [02:31<02:32, 567kB/s]\n",
      " 52%|█████▏    | 87.0M/169M [02:33<02:23, 594kB/s]\n",
      " 52%|█████▏    | 88.0M/169M [02:34<02:17, 616kB/s]\n",
      " 53%|█████▎    | 89.0M/169M [02:37<02:31, 551kB/s]\n",
      " 53%|█████▎    | 90.0M/169M [02:38<02:19, 592kB/s]\n",
      " 54%|█████▍    | 91.0M/169M [02:40<02:11, 620kB/s]\n",
      " 55%|█████▍    | 92.0M/169M [02:41<02:06, 634kB/s]\n",
      " 55%|█████▌    | 93.0M/169M [02:43<02:01, 653kB/s]\n",
      " 56%|█████▌    | 94.0M/169M [02:44<01:57, 665kB/s]\n",
      " 56%|█████▋    | 95.0M/169M [02:46<02:04, 622kB/s]\n",
      " 57%|█████▋    | 96.0M/169M [02:48<02:15, 560kB/s]\n",
      " 58%|█████▊    | 97.0M/169M [02:50<02:10, 574kB/s]\n",
      " 58%|█████▊    | 98.0M/169M [02:52<02:04, 595kB/s]\n",
      " 59%|█████▊    | 99.0M/169M [02:53<02:00, 605kB/s]\n",
      " 59%|█████▉    | 100M/169M [02:55<01:53, 633kB/s] \n",
      " 60%|█████▉    | 101M/169M [02:57<02:08, 550kB/s]\n",
      " 61%|██████    | 102M/169M [03:01<02:39, 437kB/s]\n",
      " 61%|██████    | 103M/169M [03:04<02:56, 389kB/s]\n",
      " 62%|██████▏   | 104M/169M [03:06<02:37, 429kB/s]\n",
      " 62%|██████▏   | 105M/169M [03:08<02:20, 475kB/s]\n",
      " 63%|██████▎   | 106M/169M [03:10<02:22, 461kB/s]\n",
      " 63%|██████▎   | 107M/169M [03:14<02:50, 380kB/s]\n",
      " 64%|██████▍   | 108M/169M [03:19<03:18, 320kB/s]\n",
      " 65%|██████▍   | 109M/169M [03:29<05:11, 200kB/s]\n",
      " 65%|██████▌   | 110M/169M [03:35<05:22, 191kB/s]\n",
      " 66%|██████▌   | 111M/169M [03:40<05:22, 187kB/s]\n",
      " 66%|██████▋   | 112M/169M [03:45<05:02, 196kB/s]\n",
      " 67%|██████▋   | 113M/169M [03:53<05:42, 170kB/s]\n",
      " 68%|██████▊   | 114M/169M [04:06<07:22, 129kB/s]\n",
      " 68%|██████▊   | 115M/169M [04:11<06:18, 148kB/s]\n",
      " 69%|██████▉   | 116M/169M [04:14<05:11, 177kB/s]\n",
      " 69%|██████▉   | 117M/169M [04:18<04:39, 194kB/s]\n",
      " 70%|██████▉   | 118M/169M [04:23<04:28, 198kB/s]\n",
      " 71%|███████   | 119M/169M [04:29<04:26, 195kB/s]\n",
      " 71%|███████   | 120M/169M [04:34<04:24, 193kB/s]\n",
      " 72%|███████▏  | 121M/169M [04:37<03:38, 228kB/s]\n",
      " 72%|███████▏  | 122M/169M [04:39<02:54, 280kB/s]\n",
      " 73%|███████▎  | 123M/169M [04:40<02:23, 334kB/s]\n",
      " 74%|███████▎  | 124M/169M [04:42<01:58, 393kB/s]\n",
      " 74%|███████▍  | 125M/169M [04:44<01:42, 447kB/s]\n",
      " 75%|███████▍  | 126M/169M [04:45<01:30, 494kB/s]\n",
      " 75%|███████▌  | 127M/169M [04:47<01:20, 542kB/s]\n",
      " 76%|███████▌  | 128M/169M [04:49<01:18, 541kB/s]\n",
      " 77%|███████▋  | 129M/169M [04:50<01:14, 558kB/s]\n",
      " 77%|███████▋  | 130M/169M [04:52<01:09, 585kB/s]\n",
      " 78%|███████▊  | 131M/169M [04:56<01:30, 438kB/s]\n",
      " 78%|███████▊  | 132M/169M [05:03<02:22, 269kB/s]\n",
      " 79%|███████▉  | 133M/169M [05:08<02:29, 249kB/s]\n",
      " 79%|███████▉  | 134M/169M [05:10<02:00, 302kB/s]\n",
      " 80%|████████  | 135M/169M [05:12<01:40, 350kB/s]\n",
      " 81%|████████  | 136M/169M [05:14<01:29, 382kB/s]\n",
      " 81%|████████▏ | 137M/169M [05:16<01:19, 416kB/s]\n",
      " 82%|████████▏ | 138M/169M [05:18<01:12, 442kB/s]\n",
      " 82%|████████▏ | 139M/169M [05:20<01:05, 476kB/s]\n",
      " 83%|████████▎ | 140M/169M [05:22<01:00, 496kB/s]\n",
      " 84%|████████▎ | 141M/169M [05:23<00:53, 544kB/s]\n",
      " 84%|████████▍ | 142M/169M [05:25<00:49, 567kB/s]\n",
      " 85%|████████▍ | 143M/169M [05:26<00:45, 589kB/s]\n",
      " 85%|████████▌ | 144M/169M [05:28<00:42, 605kB/s]\n",
      " 86%|████████▌ | 145M/169M [05:30<00:41, 599kB/s]\n",
      " 87%|████████▋ | 146M/169M [05:31<00:37, 626kB/s]\n",
      " 87%|████████▋ | 147M/169M [05:33<00:36, 625kB/s]\n",
      " 88%|████████▊ | 148M/169M [05:34<00:33, 647kB/s]\n",
      " 88%|████████▊ | 149M/169M [05:37<00:35, 578kB/s]\n",
      " 89%|████████▉ | 150M/169M [05:39<00:37, 516kB/s]\n",
      " 90%|████████▉ | 151M/169M [05:41<00:33, 547kB/s]\n",
      " 90%|█████████ | 152M/169M [05:43<00:31, 553kB/s]\n",
      " 91%|█████████ | 153M/169M [05:44<00:28, 576kB/s]\n",
      " 91%|█████████▏| 154M/169M [05:46<00:25, 594kB/s]\n",
      " 92%|█████████▏| 155M/169M [05:48<00:24, 583kB/s]\n",
      " 93%|█████████▎| 156M/169M [05:50<00:22, 586kB/s]\n",
      " 93%|█████████▎| 157M/169M [05:51<00:20, 593kB/s]\n",
      " 94%|█████████▎| 158M/169M [05:53<00:19, 581kB/s]\n",
      " 94%|█████████▍| 159M/169M [05:55<00:16, 615kB/s]\n",
      " 95%|█████████▍| 160M/169M [05:56<00:14, 620kB/s]\n",
      " 96%|█████████▌| 161M/169M [05:58<00:13, 608kB/s]\n",
      " 96%|█████████▌| 162M/169M [06:00<00:10, 630kB/s]\n",
      " 97%|█████████▋| 163M/169M [06:01<00:09, 647kB/s]\n",
      " 97%|█████████▋| 164M/169M [06:03<00:07, 658kB/s]\n",
      " 98%|█████████▊| 165M/169M [06:05<00:06, 598kB/s]\n",
      " 98%|█████████▊| 166M/169M [06:07<00:04, 603kB/s]\n",
      " 99%|█████████▉| 167M/169M [06:08<00:02, 589kB/s]\n",
      "100%|█████████▉| 168M/169M [06:10<00:00, 619kB/s]\n",
      "100%|██████████| 169M/169M [06:11<00:00, 640kB/s]\n",
      "100%|██████████| 169M/169M [06:11<00:00, 476kB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d thoughtvector/customer-support-on-twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C n'a pas de nom.\n",
      " Le num�ro de s�rie du volume est C41F-3C9A\n",
      "\n",
      " R�pertoire de c:\\Users\\tech aarohi\\AppData\\Local\\Programs\\Microsoft VS Code\n",
      "\n",
      "26-03-24  01:31    <DIR>          .\n",
      "26-03-24  01:31    <DIR>          ..\n",
      "18-03-24  01:07    <DIR>          bin\n",
      "08-03-24  15:20           135.642 chrome_100_percent.pak\n",
      "08-03-24  15:20           195.396 chrome_200_percent.pak\n",
      "08-03-24  15:50       163.792.928 Code.exe\n",
      "08-03-24  15:20               367 Code.VisualElementsManifest.xml\n",
      "21-09-19  15:28       176.772.673 customer-support-on-twitter.zip\n",
      "08-03-24  15:49         4.927.016 d3dcompiler_47.dll\n",
      "08-03-24  15:49         2.508.904 ffmpeg.dll\n",
      "08-03-24  15:20        10.717.392 icudtl.dat\n",
      "08-03-24  15:49           509.560 libEGL.dll\n",
      "08-03-24  15:50         7.637.608 libGLESv2.dll\n",
      "08-03-24  15:20         9.088.790 LICENSES.chromium.html\n",
      "18-03-24  01:07    <DIR>          locales\n",
      "18-03-24  01:07    <DIR>          policies\n",
      "18-03-24  01:07    <DIR>          resources\n",
      "08-03-24  15:20         5.312.183 resources.pak\n",
      "08-03-24  15:20           267.462 snapshot_blob.bin\n",
      "18-03-24  01:07    <DIR>          tools\n",
      "20-03-24  01:11        13.935.779 unins000.dat\n",
      "18-03-24  01:06         2.629.056 unins000.exe\n",
      "18-03-24  01:07            30.513 unins000.msg\n",
      "08-03-24  15:20           626.509 v8_context_snapshot.bin\n",
      "08-03-24  15:49         5.200.912 vk_swiftshader.dll\n",
      "08-03-24  15:20               106 vk_swiftshader_icd.json\n",
      "08-03-24  15:49           974.456 vulkan-1.dll\n",
      "              20 fichier(s)      405.263.252 octets\n",
      "               7 R�p(s)  17.179.193.344 octets libres\n"
     ]
    }
   ],
   "source": [
    "ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7-Zip 19.00 (x64) : Copyright (c) 1999-2018 Igor Pavlov : 2019-02-21\n",
      "\n",
      "Scanning the drive for archives:\n",
      "1 file, 176772673 bytes (169 MiB)\n",
      "\n",
      "Extracting archive: customer-support-on-twitter.zip\n",
      "--\n",
      "Path = customer-support-on-twitter.zip\n",
      "Type = zip\n",
      "Physical Size = 176772673\n",
      "\n",
      "Everything is Ok\n",
      "\n",
      "Files: 2\n",
      "Size:       516525998\n",
      "Compressed: 176772673\n"
     ]
    }
   ],
   "source": [
    "!7z x customer-support-on-twitter.zip \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.8.3-cp310-cp310-win_amd64.whl (7.6 MB)\n",
      "     ---------------------------------------- 7.6/7.6 MB 627.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tech aarohi\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\python\\python310\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\python\\python310\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.50.0-cp310-cp310-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 610.2 kB/s eta 0:00:00\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.2.0-cp310-cp310-win_amd64.whl (186 kB)\n",
      "     ------------------------------------ 186.7/186.7 kB 665.7 kB/s eta 0:00:00\n",
      "Collecting pillow>=8\n",
      "  Downloading pillow-10.2.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "     ---------------------------------------- 2.6/2.6 MB 676.6 kB/s eta 0:00:00\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-win_amd64.whl (56 kB)\n",
      "     -------------------------------------- 56.1/56.1 kB 580.4 kB/s eta 0:00:00\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "     ------------------------------------ 103.2/103.2 kB 659.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.0 cycler-0.12.1 fonttools-4.50.0 kiwisolver-1.4.5 matplotlib-3.8.3 pillow-10.2.0 pyparsing-3.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemin = \"C:/Users/tech aarohi/AppData/Local/Programs/Microsoft VS Code/sample.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(chemin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>inbound</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>response_tweet_id</th>\n",
       "      <th>in_response_to_tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>119237</td>\n",
       "      <td>105834</td>\n",
       "      <td>True</td>\n",
       "      <td>Wed Oct 11 06:55:44 +0000 2017</td>\n",
       "      <td>@AppleSupport causing the reply to be disregar...</td>\n",
       "      <td>119236</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119238</td>\n",
       "      <td>ChaseSupport</td>\n",
       "      <td>False</td>\n",
       "      <td>Wed Oct 11 13:25:49 +0000 2017</td>\n",
       "      <td>@105835 Your business means a lot to us. Pleas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>119239.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119239</td>\n",
       "      <td>105835</td>\n",
       "      <td>True</td>\n",
       "      <td>Wed Oct 11 13:00:09 +0000 2017</td>\n",
       "      <td>@76328 I really hope you all change but I'm su...</td>\n",
       "      <td>119238</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>119240</td>\n",
       "      <td>VirginTrains</td>\n",
       "      <td>False</td>\n",
       "      <td>Tue Oct 10 15:16:08 +0000 2017</td>\n",
       "      <td>@105836 LiveChat is online at the moment - htt...</td>\n",
       "      <td>119241</td>\n",
       "      <td>119242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>119241</td>\n",
       "      <td>105836</td>\n",
       "      <td>True</td>\n",
       "      <td>Tue Oct 10 15:17:21 +0000 2017</td>\n",
       "      <td>@VirginTrains see attached error message. I've...</td>\n",
       "      <td>119243</td>\n",
       "      <td>119240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>119330</td>\n",
       "      <td>105859</td>\n",
       "      <td>True</td>\n",
       "      <td>Wed Oct 11 13:50:42 +0000 2017</td>\n",
       "      <td>@105860 I wish Amazon had an option of where I...</td>\n",
       "      <td>119329</td>\n",
       "      <td>119331.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>119331</td>\n",
       "      <td>105860</td>\n",
       "      <td>True</td>\n",
       "      <td>Wed Oct 11 13:47:14 +0000 2017</td>\n",
       "      <td>They reschedule my shit for tomorrow https://t...</td>\n",
       "      <td>119330</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>119332</td>\n",
       "      <td>Tesco</td>\n",
       "      <td>False</td>\n",
       "      <td>Wed Oct 11 13:34:06 +0000 2017</td>\n",
       "      <td>@105861 Hey Sara, sorry to hear of the issues ...</td>\n",
       "      <td>119333</td>\n",
       "      <td>119334.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>119333</td>\n",
       "      <td>105861</td>\n",
       "      <td>True</td>\n",
       "      <td>Wed Oct 11 14:05:18 +0000 2017</td>\n",
       "      <td>@Tesco bit of both - finding the layout cumber...</td>\n",
       "      <td>119335,119336</td>\n",
       "      <td>119332.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>119335</td>\n",
       "      <td>Tesco</td>\n",
       "      <td>False</td>\n",
       "      <td>Wed Oct 11 15:38:07 +0000 2017</td>\n",
       "      <td>@105861 If that doesn't help please DM your fu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>119333.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    tweet_id     author_id  inbound                      created_at  \\\n",
       "0     119237        105834     True  Wed Oct 11 06:55:44 +0000 2017   \n",
       "1     119238  ChaseSupport    False  Wed Oct 11 13:25:49 +0000 2017   \n",
       "2     119239        105835     True  Wed Oct 11 13:00:09 +0000 2017   \n",
       "3     119240  VirginTrains    False  Tue Oct 10 15:16:08 +0000 2017   \n",
       "4     119241        105836     True  Tue Oct 10 15:17:21 +0000 2017   \n",
       "..       ...           ...      ...                             ...   \n",
       "88    119330        105859     True  Wed Oct 11 13:50:42 +0000 2017   \n",
       "89    119331        105860     True  Wed Oct 11 13:47:14 +0000 2017   \n",
       "90    119332         Tesco    False  Wed Oct 11 13:34:06 +0000 2017   \n",
       "91    119333        105861     True  Wed Oct 11 14:05:18 +0000 2017   \n",
       "92    119335         Tesco    False  Wed Oct 11 15:38:07 +0000 2017   \n",
       "\n",
       "                                                 text response_tweet_id  \\\n",
       "0   @AppleSupport causing the reply to be disregar...            119236   \n",
       "1   @105835 Your business means a lot to us. Pleas...               NaN   \n",
       "2   @76328 I really hope you all change but I'm su...            119238   \n",
       "3   @105836 LiveChat is online at the moment - htt...            119241   \n",
       "4   @VirginTrains see attached error message. I've...            119243   \n",
       "..                                                ...               ...   \n",
       "88  @105860 I wish Amazon had an option of where I...            119329   \n",
       "89  They reschedule my shit for tomorrow https://t...            119330   \n",
       "90  @105861 Hey Sara, sorry to hear of the issues ...            119333   \n",
       "91  @Tesco bit of both - finding the layout cumber...     119335,119336   \n",
       "92  @105861 If that doesn't help please DM your fu...               NaN   \n",
       "\n",
       "    in_response_to_tweet_id  \n",
       "0                       NaN  \n",
       "1                  119239.0  \n",
       "2                       NaN  \n",
       "3                  119242.0  \n",
       "4                  119240.0  \n",
       "..                      ...  \n",
       "88                 119331.0  \n",
       "89                      NaN  \n",
       "90                 119334.0  \n",
       "91                 119332.0  \n",
       "92                 119333.0  \n",
       "\n",
       "[93 rows x 7 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     @AppleSupport causing the reply to be disregar...\n",
       "1     @105835 Your business means a lot to us. Pleas...\n",
       "2     @76328 I really hope you all change but I'm su...\n",
       "3     @105836 LiveChat is online at the moment - htt...\n",
       "4     @VirginTrains see attached error message. I've...\n",
       "                            ...                        \n",
       "88    @105860 I wish Amazon had an option of where I...\n",
       "89    They reschedule my shit for tomorrow https://t...\n",
       "90    @105861 Hey Sara, sorry to hear of the issues ...\n",
       "91    @Tesco bit of both - finding the layout cumber...\n",
       "92    @105861 If that doesn't help please DM your fu...\n",
       "Name: text, Length: 93, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textes=df[\"text\"]\n",
    "textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\python\\python310\\lib\\site-packages (3.8.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tqdm in c:\\python\\python310\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: click in c:\\python\\python310\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python\\python310\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: joblib in c:\\python\\python310\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: colorama in c:\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\tech\n",
      "[nltk_data]     aarohi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\tech\n",
      "[nltk_data]     aarohi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@AppleSupport causing the reply to be disregarded and the tapped notification under the keyboard is opened😡😡😡\n",
      "@105835 Your business means a lot to us. Please DM your name, zip code and additional details about your concern. ^RR https://t.co/znUu1VJn9r\n"
     ]
    }
   ],
   "source": [
    "text_column_name = 'text'\n",
    "\n",
    "text_list = df[text_column_name].tolist()\n",
    "print(text_list[0])\n",
    "print(text_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@applesupport causing the reply to be disregarded and the tapped notification under the keyboard is opened😡😡😡\n",
      "@105835 your business means a lot to us. please dm your name, zip code and additional details about your concern. ^rr https://t.co/znuu1vjn9r\n"
     ]
    }
   ],
   "source": [
    "#normalisation\n",
    "text_list = [text.lower() for text in text_list]\n",
    "print(text_list[0])\n",
    "print(text_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', 'applesupport', 'causing', 'the', 'reply', 'to', 'be', 'disregarded', 'and', 'the', 'tapped', 'notification', 'under', 'the', 'keyboard', 'is', 'opened😡😡😡']\n",
      "['@', '105835', 'your', 'business', 'means', 'a', 'lot', 'to', 'us', '.', 'please', 'dm', 'your', 'name', ',', 'zip', 'code', 'and', 'additional', 'details', 'about', 'your', 'concern', '.', '^rr', 'https', ':', '//t.co/znuu1vjn9r']\n"
     ]
    }
   ],
   "source": [
    "#tokenization\n",
    "tokenized_text = [nltk.tokenize.word_tokenize(text) for text in text_list]\n",
    "print(tokenized_text[0])\n",
    "print(tokenized_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['applesupport', 'causing', 'the', 'reply', 'to', 'be', 'disregarded', 'and', 'the', 'tapped', 'notification', 'under', 'the', 'keyboard', 'is', 'opened😡😡😡']\n",
      "['105835', 'your', 'business', 'means', 'a', 'lot', 'to', 'us', 'please', 'dm', 'your', 'name', 'zip', 'code', 'and', 'additional', 'details', 'about', 'your', 'concern', '^rr', 'https', '//t.co/znuu1vjn9r']\n",
      "['105836', 'livechat', 'is', 'online', 'at', 'the', 'moment', 'https', '//t.co/sy94vtu8kq', 'or', 'contact', '03331', '031', '031', 'option', '1', '4', '3', 'leave', 'a', 'message', 'to', 'request', 'a', 'call', 'back']\n"
     ]
    }
   ],
   "source": [
    "cleaned_tokenized_text = [[token for token in text if token not in string.punctuation] for text in tokenized_text]\n",
    "\n",
    "print(cleaned_tokenized_text[0])\n",
    "print(cleaned_tokenized_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['applesupport', 'causing', 'the', 'reply', 'to', 'be', 'disregarded', 'and', 'the', 'tapped', 'notification', 'under', 'the', 'keyboard', 'is', 'opened😡😡😡']\n",
      "['your', 'business', 'means', 'a', 'lot', 'to', 'us', 'please', 'dm', 'your', 'name', 'zip', 'code', 'and', 'additional', 'details', 'about', 'your', 'concern', '^rr', 'https', '//t.co/znuu1vjn9r']\n",
      "['livechat', 'is', 'online', 'at', 'the', 'moment', 'https', '//t.co/sy94vtu8kq', 'or', 'contact', 'option', 'leave', 'a', 'message', 'to', 'request', 'a', 'call', 'back']\n"
     ]
    }
   ],
   "source": [
    "cleaned_tokenized_text = [[token for token in text if not token.isnumeric()] for text in cleaned_tokenized_text]\n",
    "print(cleaned_tokenized_text[0])\n",
    "print(cleaned_tokenized_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['applesupport', 'causing', 'reply', 'disregarded', 'tapped', 'notification', 'keyboard', 'opened😡😡😡']\n",
      "['business', 'means', 'lot', 'us', 'please', 'dm', 'name', 'zip', 'code', 'additional', 'details', 'concern', '^rr', 'https', '//t.co/znuu1vjn9r']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "cleaned_tokenized_text = [[token for token in text if token not in stopwords.words('english')] for text in cleaned_tokenized_text]\n",
    "\n",
    "\n",
    "print(cleaned_tokenized_text[0])\n",
    "print(cleaned_tokenized_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['applesupport', 'caus', 'repli', 'disregard', 'tap', 'notif', 'keyboard', 'opened😡😡😡']\n",
      "['busi', 'mean', 'lot', 'us', 'pleas', 'dm', 'name', 'zip', 'code', 'addit', 'detail', 'concern', '^rr', 'http', '//t.co/znuu1vjn9r']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokenized_text = [[stemmer.stem(token) for token in text] for text in cleaned_tokenized_text]\n",
    "print(stemmed_tokenized_text[0])\n",
    "print(stemmed_tokenized_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\tech aarohi/nltk_data'\n    - 'c:\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\tech aarohi\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\tech aarohi/nltk_data'\n    - 'c:\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\tech aarohi\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n\u001b[0;32m      2\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m----> 3\u001b[0m lemmatized_tokenized_text \u001b[38;5;241m=\u001b[39m [[lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m text] \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m cleaned_tokenized_text]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(lemmatized_tokenized_text[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(lemmatized_tokenized_text[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[68], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n\u001b[0;32m      2\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m----> 3\u001b[0m lemmatized_tokenized_text \u001b[38;5;241m=\u001b[39m [[lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m text] \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m cleaned_tokenized_text]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(lemmatized_tokenized_text[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(lemmatized_tokenized_text[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[68], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n\u001b[0;32m      2\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m----> 3\u001b[0m lemmatized_tokenized_text \u001b[38;5;241m=\u001b[39m [[\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m text] \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m cleaned_tokenized_text]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(lemmatized_tokenized_text[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(lemmatized_tokenized_text[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(word, pos)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\tech aarohi/nltk_data'\n    - 'c:\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\tech aarohi\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokenized_text = [[lemmatizer.lemmatize(token) for token in text] for text in cleaned_tokenized_text]\n",
    "print(lemmatized_tokenized_text[0])\n",
    "print(lemmatized_tokenized_text[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
